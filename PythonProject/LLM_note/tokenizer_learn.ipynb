{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb9a935",
   "metadata": {},
   "source": [
    "# Tokenizer Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a7990ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 初始化一个分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ca8c5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2091, 1136, 1143, 13002, 1107, 1103, 5707, 1104, 16678, 1116, 117, 1111, 1152, 1132, 11515, 1105, 3613, 1106, 4470, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b61e55e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7750d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1790, 112, 189, 1143, 13002, 1107, 1103, 5707, 1104, 16678, 1116, 117, 1111, 1152, 1132, 11515, 1105, 3613, 1106, 4470, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"Don't meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5245628f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] Don ' t meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf2cb647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1143, 13002, 1107, 1103, 5707, 1104, 16678, 1116, 117, 1111, 1152, 1132, 11515, 1105, 3613, 1106, 4470, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38fa52c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bdb5b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##s'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1116])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02d109ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一段话\n",
    "batch_sentences = [ \n",
    "    \"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\",\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea2271a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2091, 1136, 1143, 13002, 1107, 1103, 5707, 1104, 16678, 1116, 117, 1111, 1152, 1132, 11515, 1105, 3613, 1106, 4470, 119, 102], [101, 1252, 1184, 1164, 1248, 6462, 136, 102], [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], [101, 1327, 1164, 5450, 23434, 136, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(batch_sentences)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7cbac49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2091, 1136, 1143, 13002, 1107, 1103, 5707, 1104, 16678, 1116, 117, 1111, 1152, 1132, 11515, 1105, 3613, 1106, 4470, 119, 102], [101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102, 0, 0, 0, 0, 0, 0, 0], [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(batch_sentences, padding=True)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d26637b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2091, 1136, 1143, 13002, 1107, 1103, 5707, 1104, 16678, 1116, 117, 1111, 1152, 1132, 11515, 1105, 3613, 1106, 4470, 119, 102], [101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102, 0, 0, 0, 0, 0, 0, 0], [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(batch_sentences, padding=True, truncation=True)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73d7d91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 100, 100, 100, 1096, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(\"我爱你！\")\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87c6e09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [UNK] [UNK] [UNK] ！ [SEP]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "286a485e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 100, 1010, 100, 17881, 1571, 1026, 124, 1037, 1407, 1033, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(\"今天是2025年3月18日\")\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7deb4dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [UNK] 天 [UNK] 2025 年 3 月 18 日 [SEP]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019aac2f",
   "metadata": {},
   "source": [
    "### bert-base-chinese分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbbe18a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一个bert族 chinese 的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff2dafc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2769, 4263, 872, 8013, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(\"我爱你！\")\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68107073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 我 爱 你 ！ [SEP]'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f513a120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 791, 1921, 3221, 8950, 2399, 124, 3299, 8123, 3189, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(\"今天是2025年3月18日\")\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af1c8584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 今 天 是 2025 年 3 月 18 日 [SEP]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b565e4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 736, 4636, 753, 1282, 676, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(\"九百二十三\")\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ef5d3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'九'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_inputs['input_ids'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44411a",
   "metadata": {},
   "source": [
    "### Qwen-2.5-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45815e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85345a12bf640e9b2996e2a0a1ed2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e3627b7c9a436ca3289e1707197b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5ee1c0ad154485aaba2dde4c1f82ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d243f5777c445c91094168e68b6983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d527893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[5404, 537, 1774, 90385, 304, 279, 21978, 315, 88163, 11, 369, 807, 525, 26447, 323, 3974, 311, 19234, 13], [3983, 1128, 911, 2086, 17496, 30], [8002, 944, 1744, 566, 8788, 911, 2086, 17496, 11, 77382, 13], [3838, 911, 11964, 724, 550, 30]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(batch_sentences)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dbc5734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do not meddle in the affairs of wizards, for they are subtle and quick to anger.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "563897c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [100644, 20412, 220, 17, 15, 17, 20, 74577, 112, 220, 18, 220, 9754, 220, 16, 23, 75402, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(\"今天是 2025 年 3 月 18 日!\")\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ad9a4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今天是 2025 年 3 月 18 日!'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecbab314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [102836, 40820, 100957], 'attention_mask': [1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(\"一百二十三\")\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cbba0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一百'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9001d00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'二'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_inputs['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2199fa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [99609, 99271, 40820, 100957], 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(\"九百二十三\")\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21372e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'九'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9ba5f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'百'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_inputs['input_ids'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6c6124",
   "metadata": {},
   "source": [
    "### Qwen2.5-0.5B-Instruct\n",
    "我突然就有一个问题，Qwen-2.5-7B-Instruct 和 Qwen2.5-0.5B-Instruct 使用的是一个Tokenizer吗？我觉得应该是一个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aeec5f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d3a5ad1aa748e7b03d41205fb8de19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cf9acc0008460a8fdb760fdaaf8630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb85cfc822942c8b3a3bdfb2443b614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b491ed31ad074175b814b66caf54e4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9f07d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [100644, 20412, 220, 17, 15, 17, 20, 74577, 112, 220, 18, 220, 9754, 220, 16, 23, 75402, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(\"今天是 2025 年 3 月 18 日!\")\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f6418",
   "metadata": {},
   "source": [
    "看来Qwen2.5使用的都是一个分词器;该分词器是一个多语言的分词器（至少包含英语和汉语）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "781ff3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今天是 2025 年 3 月 18 日!'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "797d204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [99609, 99271, 40820, 100957], 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(\"九百二十三\")\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac2f0f3",
   "metadata": {},
   "source": [
    "### Qwen2-0.5B-Instruct\n",
    "我又想到一个新的问题，Qwen2.5-7B-Instruct和Qwen2.5-0.5B-Instruct使用的是一个Tokenizer，这是因为它们都是Qwen2.5系列的；那么Qwen2和Qwen2.5之间的Tokenizer是否是一样的？我认为可能是不一样的；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3e0b740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f953b4d26748a2a3bbb5bcd47e25ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612f13fea44f443db0ddf9764b7e1402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d4a58c9c524773b1a9d3ed0f460bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a4b3eb5fb941fa9435397d5296cfc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7bc290a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [100644, 20412, 220, 17, 15, 17, 20, 74577, 112, 220, 18, 220, 9754, 220, 16, 23, 75402, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(\"今天是 2025 年 3 月 18 日!\")\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015238ee",
   "metadata": {},
   "source": [
    "Qwen2.5-0.5B:{'input_ids': [100644, 20412, 220, 17, 15, 17, 20, 74577, 112, 220, 18, 220, 9754, 220, 16, 23, 75402, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "看起来是一样的！Qwen2和Qwen2.5的Tokenizer应该是一样的？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8258e0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今天是 2025 年 3 月 18 日!'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7adfb7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [99609, 99271, 40820, 100957], 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(\"九百二十三\")\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49401f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ä¹Ŀ', 'çĻ¾', 'äºĮ', 'åįģä¸ī']\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer.tokenize(\"九百二十三\")\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3eb5386c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[99609, 99271, 40820, 100957]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58e16e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
